# -*- coding: utf-8 -*-
"""ImageOnly_Zoobot_MaxViT_v3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pnE28iMgejQmtykYeihsYI4TahSfOBjY
"""
class ImageOnlyZoobotModelMaxViT(pl.LightningModule):

    def __init__(self, name: str, num_classes: int):
        super().__init__()

        # timm 라이브러리로 pretrained 모델 생성, num_classes=0으로 feature extractor로 설정
        self.encoder = timm.create_model(name, pretrained=True, num_classes=0, drop_rate=0.2, drop_path_rate=0.1)

        # 더미 입력으로 encoder의 feature 차원 확인
        dummy_input = torch.zeros(1, 3, 224, 224)
        with torch.no_grad():
            dummy_output = self.encoder.forward_features(dummy_input)

        # feature vector 크기 계산 (배치 제외)
        feature_dim = dummy_output.reshape(1, -1).size(1)

        # 분류기 헤드 정의: feature_dim -> num_classes
        self.head = nn.Linear(feature_dim, num_classes)

        # 손실 함수 정의 (CrossEntropyLoss)
        self.loss_fn = nn.CrossEntropyLoss()

        # 학습 및 검증 성능 지표 기록용 리스트 초기화
        self.train_losses, self.val_losses = [], []
        self.train_accs, self.val_accs = [], []
        self.train_f1s, self.val_f1s = [], []

        # 에폭별 시간 기록용 리스트 및 변수 초기화
        self.epoch_times = []
        self.train_start_time = None
        self.epoch_start_time = None


    def forward(self, image):
        # 이미지에서 feature 추출
        features = self.encoder.forward_features(image)
        # batch 차원 유지하고 1차원으로 펼침
        features = features.reshape(features.size(0), -1)
        # 분류기 헤드를 통해 최종 출력 생성
        return self.head(features)


    def training_step(self, batch, batch_idx):
        image, label = batch

        # 모델 예측
        logits = self(image)

        # 손실 계산
        loss = self.loss_fn(logits, label)

        # 예측 클래스 계산
        preds = logits.argmax(dim=1)

        # 정확도 계산
        acc = (preds == label).float().mean()

        # F1 score (macro) 계산 (cpu로 이동 후 numpy 연산)
        f1 = f1_score(label.cpu(), preds.cpu(), average='macro')

        # 학습 스텝 결과 저장 리스트 초기화
        if not hasattr(self, 'train_step_outputs'):
            self.train_step_outputs = []

        # 현재 스텝 결과 저장 (loss, acc, f1)
        self.train_step_outputs.append({
            'loss': loss.detach(),
            'acc': acc.detach(),
            'f1': torch.tensor(f1)
        })

        # 로그 기록 (TensorBoard, progress bar 출력용)
        self.log("train_loss", loss, prog_bar=True)
        self.log("train_acc", acc, prog_bar=True)
        self.log("train_f1", f1, prog_bar=True)

        return loss


    def on_train_start(self):
        # 학습 시작 시간 기록 및 알림 출력
        self.train_start_time = time.time()
        print("\n=== Training Started ===")


    def on_train_epoch_start(self):
        # 한 에폭 시작 시간 기록
        self.epoch_start_time = time.time()


    def on_train_epoch_end(self):
        # 한 에폭 동안 기록한 모든 배치 결과 가져오기
        outputs = self.train_step_outputs

        # 배치별 손실, 정확도, f1 평균 계산
        avg_loss = torch.stack([x['loss'] for x in outputs]).mean().item()
        avg_acc = torch.stack([x['acc'] for x in outputs]).mean().item()
        avg_f1 = torch.stack([x['f1'] for x in outputs]).mean().item()

        # 각 에폭 결과 저장
        self.train_losses.append(avg_loss)
        self.train_accs.append(avg_acc)
        self.train_f1s.append(avg_f1)

        # 에폭 시간 측정 및 저장
        epoch_duration = time.time() - self.epoch_start_time
        self.epoch_times.append(epoch_duration)

        # 결과 출력
        print(f"Epoch {self.current_epoch} time: {epoch_duration:.2f} sec")

        # 저장한 스텝 결과 초기화
        self.train_step_outputs.clear()


    def validation_step(self, batch, batch_idx):
        image, label = batch

        # 모델 예측
        logits = self(image)

        # 손실 계산
        loss = self.loss_fn(logits, label)

        # 예측 클래스
        preds = logits.argmax(dim=1)

        # 정확도 계산
        acc = (preds == label).float().mean()

        # F1 score 계산 (macro)
        f1 = f1_score(label.cpu(), preds.cpu(), average='macro')

        # 검증 스텝 결과 저장 리스트 초기화
        if not hasattr(self, 'val_step_outputs'):
            self.val_step_outputs = []

        # 현재 검증 스텝 결과 저장
        self.val_step_outputs.append({
            'val_loss': loss.detach(),
            'val_acc': acc.detach(),
            'val_f1': torch.tensor(f1)
        })

        # 로그 기록 (프로그레스바 출력)
        self.log("val_loss", loss, prog_bar=True)
        self.log("val_acc", acc, prog_bar=True)
        self.log("val_f1", f1, prog_bar=True)

        return loss


    def on_validation_epoch_end(self):
        # 모든 검증 스텝 결과 수집
        outputs = self.val_step_outputs

        # 평균 손실, 정확도, f1 계산
        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean().item()
        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean().item()
        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean().item()

        # 검증 결과 저장
        self.val_losses.append(avg_loss)
        self.val_accs.append(avg_acc)
        self.val_f1s.append(avg_f1)

        # 저장한 스텝 결과 초기화
        self.val_step_outputs.clear()


    def on_train_end(self):
        # 학습 종료 후 전체 시간 출력
        total_training_time = time.time() - self.train_start_time
        print(f"\n=== Training Finished in {total_training_time:.2f} seconds ===")

        # 각 에폭별 시간 출력
        for i, t in enumerate(self.epoch_times):
            print(f"Epoch {i}: {t:.2f} seconds")


    def configure_optimizers(self):
        # Adam 옵티마이저 정의 (학습률 0.001)
        optimizer = Adam(self.parameters(), lr=0.001)

        # ReduceLROnPlateau 스케줄러 정의 (val_loss 개선 없으면 LR 감소)
        scheduler = {
            'scheduler': ReduceLROnPlateau(
                optimizer, mode='min', factor=0.5, patience=3, verbose=True
            ),
            'monitor': 'val_loss'
        }

        # 옵티마이저와 스케줄러 반환
        return {"optimizer": optimizer, "lr_scheduler": scheduler}
